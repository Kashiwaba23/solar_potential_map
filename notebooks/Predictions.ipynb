{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe262c0c-d166-476e-88f5-e1c97dd92e3f",
   "metadata": {},
   "source": [
    "# Welcome to predicting!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8b4574-18f3-4aa6-a700-32a2137ffd29",
   "metadata": {},
   "source": [
    "## If in collab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e06066-3c64-4e95-a95e-4d823ea5b237",
   "metadata": {},
   "source": [
    "Mount in drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9011012f-4b96-4aab-92bc-439f4c2199b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6319b5f1-89a2-4e2b-8239-6e54f601cda4",
   "metadata": {},
   "source": [
    "Allow access to gcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249bc3e3-457d-45e1-87b1-022208605439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0851bed-a0f7-46d2-a719-9fe903797550",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba310702-f196-457a-937b-6f444b301fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from scipy import ndimage\n",
    "from datasets import Dataset\n",
    "from PIL import Image\n",
    "from google.cloud import storage\n",
    "from transformers import SamModel, SamConfig, SamProcessor\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f48ed6d-9ec2-4666-9c41-fb9c60890922",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a590386-5614-4c64-a20a-b46bccce7af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model configuration\n",
    "model_config = SamConfig.from_pretrained(\"facebook/sam-vit-base\")\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "# Create an instance of the model architecture with the loaded configuration\n",
    "solar_model = SamModel(config=model_config)\n",
    "#Update the model by loading the weights from saved file.\n",
    "solar_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab_Notebooks/final_project/model_checkpoint/fully_trained.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd75b2b5-75ed-4351-a8eb-281cefa4247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "solar_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163172d0-63d6-4b0d-9c06-b27ac1a6ce15",
   "metadata": {},
   "source": [
    "## Data leakage vers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bc6d4a-b072-4916-8061-1c8500579329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a random training example\n",
    "idx = random.randint(0, filtered_images.shape[0]-1)\n",
    "\n",
    "# load image\n",
    "test_image = dataset[idx][\"image\"]\n",
    "\n",
    "# get box prompt based on ground truth segmentation map\n",
    "ground_truth_mask = np.array(dataset[idx][\"label\"])\n",
    "prompt = get_bounding_box(ground_truth_mask)\n",
    "\n",
    "# prepare image + box prompt for the model\n",
    "inputs = processor(test_image, input_boxes=[[prompt]], return_tensors=\"pt\")\n",
    "\n",
    "# Move the input tensor to the GPU if it's not already there\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "solar_model.eval()\n",
    "\n",
    "# forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = solar_model(**inputs, multimask_output=False)\n",
    "\n",
    "# apply sigmoid\n",
    "medsam_seg_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
    "# convert soft mask to hard mask\n",
    "medsam_seg_prob = medsam_seg_prob.cpu().numpy().squeeze()\n",
    "medsam_seg = (medsam_seg_prob > 0.5).astype(np.uint8)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot the first image on the left\n",
    "axes[0].imshow(np.array(test_image), cmap='gray')  # Assuming the first image is grayscale\n",
    "axes[0].set_title(\"Image\")\n",
    "\n",
    "# Plot the second image on the right\n",
    "axes[1].imshow(medsam_seg, cmap='gray')  # Assuming the second image is grayscale\n",
    "axes[1].set_title(\"Mask\")\n",
    "\n",
    "# Plot the second image on the right\n",
    "axes[2].imshow(medsam_seg_prob)  # Assuming the second image is grayscale\n",
    "axes[2].set_title(\"Probability Map\")\n",
    "\n",
    "# Hide axis ticks and labels\n",
    "for ax in axes:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "# Display the images side by side\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fa72e1-a677-49ee-b7cb-a16efa1c76d5",
   "metadata": {},
   "source": [
    "## Google maps data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1b1c87-3000-425f-95a5-608626b41eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = storage.Client()  # Implicit environ set-up\n",
    "training_bucket = client.get_bucket('training_images_solar')\n",
    "\n",
    "def import_data(folder, bucket) -> np.array:\n",
    "    \"\"\"\n",
    "    Import images from specified Google Cloud folder\n",
    "    Returns a list of arrays\n",
    "    \"\"\"\n",
    "    \n",
    "    #get all file names\n",
    "    file_names = [blob.name for blob in bucket.list_blobs(prefix=folder)]\n",
    "    #sort file names\n",
    "    file_names_sorted = sorted(file_names)\n",
    "\n",
    "    #storing all the np.array images\n",
    "    folder_images = []\n",
    "\n",
    "    i = 1\n",
    "    for file in file_names:\n",
    "        #import each picture from folder\n",
    "        blob = bucket.blob(file)\n",
    "        as_bytes = blob.download_as_bytes()\n",
    "        im = Image.open(BytesIO(as_bytes))\n",
    "        #resize w pillow!!\n",
    "        im = im.resize((256, 256))\n",
    "        # #append to list as an array\n",
    "        folder_images.append(np.asarray(im, dtype=np.uint8))\n",
    "\n",
    "        #just a counter to see how fast it's moving\n",
    "        print(f\"Completed {i} images\")\n",
    "        i+=1\n",
    "\n",
    "    return folder_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e7b1ce-8503-43a9-be60-e10ad7230f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = import_data(folder='google_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd810e28-e0d6-4dc2-bd6e-d198dcbe8597",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4269866d-4a6e-4d90-a716-98a551eed794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size of your array\n",
    "array_size = img.shape\n",
    "\n",
    "# Define the size of your grid\n",
    "grid_size = 10\n",
    "\n",
    "# Generate the grid points\n",
    "x = np.linspace(0, array_size-1, grid_size)\n",
    "y = np.linspace(0, array_size-1, grid_size)\n",
    "\n",
    "# Generate a grid of coordinates\n",
    "xv, yv = np.meshgrid(x, y)\n",
    "\n",
    "# Convert the numpy arrays to lists\n",
    "xv_list = xv.tolist()\n",
    "yv_list = yv.tolist()\n",
    "\n",
    "# Combine the x and y coordinates into a list of list of lists\n",
    "input_points = [[[int(x), int(y)] for x, y in zip(x_row, y_row)] for x_row, y_row in zip(xv_list, yv_list)]\n",
    "\n",
    "#We need to reshape our nxn grid to the expected shape of the input_points tensor\n",
    "# (batch_size, point_batch_size, num_points_per_image, 2),\n",
    "# where the last dimension of 2 represents the x and y coordinates of each point.\n",
    "#batch_size: The number of images you're processing at once.\n",
    "#point_batch_size: The number of point sets you have for each image.\n",
    "#num_points_per_image: The number of points in each set.\n",
    "input_points = torch.tensor(input_points).view(1, 1, grid_size*grid_size, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1e3353-881b-4ec7-a8b2-e8f4af520326",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(input_points).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0f9fd5-7dba-406e-8f71-9ba0decc7b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random patch for segmentation\n",
    "\n",
    "# Compute the total number of 256x256 arrays\n",
    "#num_arrays = patches.shape[0] * patches.shape[1]\n",
    "# Select a random index\n",
    "#index = np.random.choice(num_arrays)\n",
    "# Compute the indices in the original array\n",
    "#i = index // patches.shape[1]\n",
    "#j = index % patches.shape[1]\n",
    "\n",
    "#Or pick a specific patch for study.\n",
    "i, j = 1, 2\n",
    "\n",
    "# Selected patch for segmentation\n",
    "random_array = patches[i, j]\n",
    "\n",
    "\n",
    "single_patch = Image.fromarray(random_array)\n",
    "# prepare image for the model\n",
    "\n",
    "#First try without providing any prompt (no bounding box or input_points)\n",
    "#inputs = processor(single_patch, return_tensors=\"pt\")\n",
    "#Now try with bounding boxes. Remember to uncomment.\n",
    "inputs = processor(single_patch, input_points=input_points, return_tensors=\"pt\")\n",
    "\n",
    "# Move the input tensor to the GPU if it's not already there\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "solar_model.eval()\n",
    "\n",
    "\n",
    "# forward pass\n",
    "with torch.no_grad():\n",
    "  outputs = solar_model(**inputs, multimask_output=False)\n",
    "\n",
    "# apply sigmoid\n",
    "single_patch_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
    "# convert soft mask to hard mask\n",
    "single_patch_prob = single_patch_prob.cpu().numpy().squeeze()\n",
    "single_patch_prediction = (single_patch_prob > 0.5).astype(np.uint8)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot the first image on the left\n",
    "axes[0].imshow(np.array(single_patch), cmap='gray')  # Assuming the first image is grayscale\n",
    "axes[0].set_title(\"Image\")\n",
    "\n",
    "# Plot the second image on the right\n",
    "axes[1].imshow(single_patch_prob)  # Assuming the second image is grayscale\n",
    "axes[1].set_title(\"Probability Map\")\n",
    "\n",
    "# Plot the second image on the right\n",
    "axes[2].imshow(single_patch_prediction, cmap='gray')  # Assuming the second image is grayscale\n",
    "axes[2].set_title(\"Prediction\")\n",
    "\n",
    "# Hide axis ticks and labels\n",
    "for ax in axes:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "# Display the images side by side\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
