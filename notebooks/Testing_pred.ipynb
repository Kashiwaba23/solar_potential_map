{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a86e2818-94dc-4bc6-8747-b482340a7f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-24 17:34:19.362739: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-24 17:34:19.362827: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-24 17:34:19.514561: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-24 17:34:19.820200: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-24 17:34:23.334140: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import SamModel, SamConfig, SamProcessor\n",
    "import torch\n",
    "import numpy as np\n",
    "from google.cloud import storage\n",
    "from PIL import Image\n",
    "import os \n",
    "from google.cloud import storage\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74378c26-6484-4180-9847-d155d85f2763",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = storage.Client()\n",
    "training_bucket = client.get_bucket('training_images_solar')\n",
    "\n",
    "def import_data(folder, bucket) -> np.array:\n",
    "    \"\"\"\n",
    "    Import images from specified Google Cloud folder\n",
    "    Returns a list of arrays\n",
    "    \"\"\"\n",
    "\n",
    "    #get all file names\n",
    "    file_names = [blob.name for blob in bucket.list_blobs(prefix=folder)]\n",
    "\n",
    "    #storing all the np.array images\n",
    "    folder_images = []\n",
    "\n",
    "    i = 1\n",
    "    for file in file_names:\n",
    "        #import each picture from folder\n",
    "        blob = bucket.blob(file)\n",
    "        as_bytes = blob.download_as_bytes()\n",
    "        im = Image.open(BytesIO(as_bytes))\n",
    "        # #append to list as an array\n",
    "        folder_images.append(np.asarray(im, dtype=np.uint8))\n",
    "\n",
    "        #just a counter to see how fast it's moving\n",
    "        print(f\"Completed {i} images\")\n",
    "        i+=1\n",
    "\n",
    "    return folder_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e8b9e01-b63c-45e2-ac32-c8646cea8a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 1 images\n",
      "Completed 2 images\n",
      "Completed 3 images\n",
      "Completed 4 images\n",
      "Completed 5 images\n",
      "Completed 6 images\n",
      "Completed 7 images\n",
      "Completed 8 images\n",
      "Completed 9 images\n",
      "Completed 10 images\n",
      "Completed 11 images\n",
      "Completed 12 images\n",
      "Completed 13 images\n",
      "Completed 14 images\n",
      "Completed 15 images\n",
      "Completed 16 images\n",
      "Completed 17 images\n",
      "Completed 18 images\n",
      "Completed 19 images\n",
      "Completed 20 images\n",
      "Completed 21 images\n",
      "Completed 22 images\n",
      "Completed 23 images\n",
      "Completed 24 images\n",
      "Completed 25 images\n",
      "Completed 26 images\n",
      "Completed 27 images\n",
      "Completed 28 images\n",
      "Completed 29 images\n",
      "Completed 30 images\n",
      "Completed 31 images\n",
      "Completed 32 images\n",
      "Completed 33 images\n",
      "Completed 34 images\n",
      "Completed 35 images\n",
      "Completed 36 images\n",
      "Completed 37 images\n",
      "Completed 38 images\n",
      "Completed 39 images\n",
      "Completed 40 images\n",
      "Completed 41 images\n",
      "Completed 42 images\n",
      "Completed 43 images\n",
      "Completed 44 images\n",
      "Completed 45 images\n",
      "Completed 46 images\n",
      "Completed 47 images\n",
      "Completed 48 images\n",
      "Completed 49 images\n",
      "Completed 50 images\n",
      "Completed 51 images\n",
      "Completed 52 images\n",
      "Completed 53 images\n",
      "Completed 54 images\n",
      "Completed 55 images\n",
      "Completed 56 images\n",
      "Completed 57 images\n",
      "Completed 58 images\n",
      "Completed 59 images\n",
      "Completed 60 images\n",
      "Completed 61 images\n",
      "Completed 62 images\n",
      "Completed 63 images\n",
      "Completed 64 images\n",
      "Completed 65 images\n",
      "Completed 66 images\n",
      "Completed 67 images\n",
      "Completed 68 images\n",
      "Completed 69 images\n",
      "Completed 70 images\n",
      "Completed 71 images\n",
      "Completed 72 images\n",
      "Completed 73 images\n",
      "Completed 74 images\n",
      "Completed 75 images\n",
      "Completed 76 images\n",
      "Completed 77 images\n",
      "Completed 78 images\n",
      "Completed 79 images\n",
      "Completed 80 images\n",
      "Completed 81 images\n",
      "Completed 82 images\n",
      "Completed 83 images\n",
      "Completed 84 images\n",
      "Completed 85 images\n",
      "Completed 86 images\n",
      "Completed 87 images\n",
      "Completed 88 images\n",
      "Completed 89 images\n",
      "Completed 90 images\n",
      "Completed 91 images\n",
      "Completed 92 images\n",
      "Completed 93 images\n",
      "Completed 94 images\n",
      "Completed 95 images\n",
      "Completed 96 images\n",
      "Completed 97 images\n",
      "Completed 98 images\n",
      "Completed 99 images\n",
      "Completed 100 images\n"
     ]
    }
   ],
   "source": [
    "test = import_data(folder='google_images', bucket=training_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09c7b67a-dc3a-4a5e-850d-1163cd74a1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = test[44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4b58255-e612-4aa6-b215-f0871b25d3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    \"\"\"Downloads retrained SAM model weights from google cloud bucket and\n",
    "    returns model ready to make a prediction\"\"\"\n",
    "    # Load the model configuration\n",
    "    model_config = SamConfig.from_pretrained(\"facebook/sam-vit-base\")\n",
    "    # processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "    dir = os.getcwd()\n",
    "    # Load the model configuration\n",
    "    # Create an instance of the model architecture with the loaded configuration\n",
    "    model = SamModel(config=model_config)\n",
    "    model.load_state_dict(torch.load(f'{dir}/../model/fully_trained.pth', map_location=torch.device('cpu')))\n",
    "    # device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf2f2fa5-e811-4894-8767-79ae39283d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a79ca43-a969-4bf4-8421-9ee804cdc9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mask(model, image) -> np.array:\n",
    "    \"\"\"Takes in a retrained SAM and a google earth\n",
    "    satellite image in .png format of any size and outputs a black and white image\n",
    "    corresponding to rooftop masks. Output size is xxx by xxx.\"\"\"\n",
    "    processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "    # image_array = np.asarray(Image.open(image))\n",
    "    # The google images seem to have a 4th alpha dimension, so we need to\n",
    "    # cut it off with splicing\n",
    "    image = image[:, :, :-1]\n",
    "    array_size = image.shape[0]\n",
    "    # Higher grid sizes seem to confuse the model and decrease performance\n",
    "    grid_size = 10\n",
    "    # Generate grid points which will serve as prompt for SAM\n",
    "    x = np.linspace(0, array_size-1, grid_size)\n",
    "    y = np.linspace(0, array_size-1, grid_size)\n",
    "    # Generate a grid of coordinates\n",
    "    xv, yv = np.meshgrid(x, y)\n",
    "    # Convert the numpy arrays to lists\n",
    "    xv_list = xv.tolist()\n",
    "    yv_list = yv.tolist()\n",
    "    # Combine the x and y coordinates into a list of list of lists\n",
    "    input_points = [[[int(x), int(y)] for x, y in zip(x_row, y_row)] for x_row, y_row in zip(xv_list, yv_list)]\n",
    "    #We need to reshape our nxn grid to the expected shape of the input_points tensor\n",
    "    # (batch_size, point_batch_size, num_points_per_image, 2),\n",
    "    # where the last dimension of 2 represents the x and y coordinates of each point.\n",
    "    #batch_size: The number of images you're processing at once.\n",
    "    #point_batch_size: The number of point sets you have for each image.\n",
    "    #num_points_per_image: The number of points in each set.\n",
    "    input_points = torch.tensor(input_points).view(1, 1, grid_size*grid_size, 2)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    inputs = processor(image, input_points=input_points, return_tensors=\"pt\")\n",
    "    # Move the input tensor to the GPU if it's not already there\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    model.eval()\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, multimask_output=False)\n",
    "    # apply sigmoid\n",
    "    mask_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
    "    # convert soft mask to hard mask\n",
    "    mask_prob = mask_prob.cpu().numpy().squeeze()\n",
    "    mask_prediction = (mask_prob > 0.5).astype(np.uint8)\n",
    "    return mask_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625970c5-4332-46ef-848d-a46a9e3ac666",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_mask(model, test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3e7eff-96d6-4077-9603-0657fe3f01e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
